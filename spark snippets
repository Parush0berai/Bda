code for making df:


val employee = sc.textFile("/workdir/employee.txt")
case class Employee(ID:Int,Name:String,Age:Int,Salary:Int)
val employeerdd=employee.map(_.split("\\s+")).map(x=>Employee(x(0).toInt,x(1),x(2).toInt,x(3).toInt))
val ds=employeerdd.toDS()
val df=employeerdd.toDF()
grouping:
val grooup=ds.groupBy("name")
groop.count().show
mapping:
val ans=employeerdd.map(x=>(x.name,x.age*2,x.salary))

filtering:
val ans2=employeerdd.filter(x=>x.salary<20000)


from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_replace

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Replace Card Type") \
    .getOrCreate()

# Read the CSV file into a DataFrame
df = spark.read.csv("/aakash/q8input.txt", header=True)

# Replace text in Card_type column using regexp_replace
df = df.withColumn("Card_type", regexp_replace(df["Card_type"], "Checking", "Cash"))

# Show the updated DataFrame
df.show()

# Stop the SparkSession
spark.stop()

spark-submit pyspark.py


//multiple delimiters:

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Multi-Delimiter DataFrame") \
    .getOrCreate()
file_path = "path/to/your/data.txt"
df = spark.read.text(file_path).toDF("line")
from pyspark.sql.functions import split, col

# Regular expression to match any of the delimiters
regex_pattern = "[ ,;|]+"

# Apply the split function and expand the result into separate columns
# Let's assume you expect 3 columns after the split
split_col = split(col("line"), regex_pattern)
df = df.withColumn("Column1", split_col.getItem(0)) \
       .withColumn("Column2", split_col.getItem(1)) \
       .withColumn("Column3", split_col.getItem(2))

# Optionally, drop the original 'line' column
df = df.drop("line")
