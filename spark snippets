code for making df:


val employee = sc.textFile("/workdir/employee.txt")
case class Employee(ID:Int,Name:String,Age:Int,Salary:Int)
val employeerdd=employee.map(_.split("\\s+")).map(x=>Employee(x(0).toInt,x(1),x(2).toInt,x(3).toInt))
val ds=employeerdd.toDS()
val df=employeerdd.toDF()
grouping:
val grooup=ds.groupBy("name")
groop.count().show
mapping:
val ans=employeerdd.map(x=>(x.name,x.age*2,x.salary))

filtering:
val ans2=employeerdd.filter(x=>x.salary<20000)


from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_replace

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Replace Card Type") \
    .getOrCreate()

# Read the CSV file into a DataFrame
df = spark.read.csv("/aakash/q8input.txt", header=True)

# Replace text in Card_type column using regexp_replace
df = df.withColumn("Card_type", regexp_replace(df["Card_type"], "Checking", "Cash"))

# Show the updated DataFrame
df.show()

# Stop the SparkSession
spark.stop()

spark-submit pyspark.py

